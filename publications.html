<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Utkarsh  Dwivedi | Publications</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Utkarsh</span>   Dwivedi
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <!-- <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li> -->
          <li class="nav-item ">
            <a class="nav-link" href="/">Home</a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/patents">
                Patents
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/bio">
                Bio/CV
                
              </a>
          </li>
          
          
          <!-- <li class="nav-item ">
            <a class="nav-link" href="/pdfs/Utkarsh_CV.pdf" download>
              Download CV
              
            </a>
        </li> -->
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h4 class="post-title">Publications</h4>
    <p class="card-text">Research papers resulting from collaborations from my time at IIT Guwahati, IBM Research India, and now at University of Maryland.</p>
  </header>

  <article>
    <div class="newprojects container mt-4">

  
  
  
  <div class="row mb-3">
    <!-- 
    <a href="/publications/chi24/">
     -->
      
      <div class="col-sm-3">
          <img class="img-fluid" src="/assets/img/storyboarding-singlecol.png" alt="An image where two children and two adults are sitting on the floor discussing their AI ideas." />
      </div>
      
      <div class="col-sm-9">
          <h4 class="card-title">Exploring AI Problem Formulation with Children via Teachable Machines</h4>
          <p class="card-text"><b>Dwivedi, U.,</b> Elsayed-Ali, S, Bonsignore, E., and Kacorri, H. Exploring AI Problem Formulation with Children via Teachable Machines. <i>ACM Conference on Human Factors in Computing Systems (CHI '24).</i></p>
          
            <p class="purplecolor"> <i class="fas fa-medal"></i> Best Paper Award, Honorable Mention</p>
          
          
          <div class="row abbr ml-1 p-0 pubs">
           <div class="links">
                <a href="https://arxiv.org/abs/2402.18688" class="btn btn-sm z-depth-0 m-0" role="button" target="_blank">Full Paper <i class="fas fa-download"></i></a>
                
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                
                </div>
                
                  <div class="abstract hidden">
                    <p>Emphasizing problem formulation in AI literacy activities with children is vital, yet we lack empirical studies on their structure and affordances. We propose that participatory design involving teachable machines facilitates problem formulation activities. To test this, we integrated problem reduction heuristics into storyboarding and invited a university-based intergenerational design team of 10 children (ages 8-13) and 9 adults to co-design a teachable machine. We find that children draw from personal experiences when formulating AI problems; they assume voice and video capabilities, explore diverse machine learning approaches, and plan for error handling. Their ideas promote human involvement in AI, though some are drawn to more autonomous systems. Their designs prioritize values like capability, logic, helpfulness, responsibility, and obedience, and a preference for a comfortable life, family security, inner harmony, and excitement as end-states. We conclude by discussing how these results can inform the design of future participatory AI activities.</p>
                  </div>
                

          </div>
          <div class="row ml-1 mr-1 p-0 pubs">
              
            
        </div>
      </div>
    <!-- </a> -->
  </div>

  
  <div class="row mb-3">
    <!-- 
    <a href="/publications/vlhcc21/">
     -->
      
      <div class="col-sm-3">
          <img class="img-fluid" src="/assets/img/ipaper2.png" alt="An image where children are demonstrating their trained models to a class. A child holds an origami while an adult holds the laptop and another adult is taking notes on a whiteboard." />
      </div>
      
      <div class="col-sm-9">
          <h4 class="card-title">Exploring Machine Teaching with Children</h4>
          <p class="card-text"><b>Dwivedi, U.,</b> Gandhi, J., Parikh, R., Coenraad, M., Bonsignore, E., and Kacorri, H. Exploring Machine Teaching with Children. <i>IEEE Symposium on Visual Languages and Human-Centric Computing (VLHCC '21).</i></p>
          
            <p class="purplecolor"> <i class="fas fa-medal"></i> Best Paper Award, Honorable Mention</p>
          
          
          <div class="row abbr ml-1 p-0 pubs">
           <div class="links">
                <a href="https://arxiv.org/abs/2109.11434" class="btn btn-sm z-depth-0 m-0" role="button" target="_blank">Full Paper <i class="fas fa-download"></i></a>
                
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                
                </div>
                
                  <div class="abstract hidden">
                    <p>Iteratively building and testing machine learning models can help children develop creativity, flexibility, and comfort with machine learning and artificial intelligence. We explore how children use machine teaching interfaces with a team of 14 children (aged 7-13 years) and adult co-designers.Children trained image classifiers and tested each other's models for robustness. Our study illuminates how children reason about ML concepts, offering these insights for designing machine teaching experiences for children - (i) ML metrics (\eg confidence scores should be visible for experimentation; (ii) ML activities should enable children to exchange models for promoting reflection and pattern recognition; and (iii) the interface should allow quick data inspection (\eg images vs. gestures).</p>
                  </div>
                

          </div>
          <div class="row ml-1 mr-1 p-0 pubs">
              
            
        </div>
      </div>
    <!-- </a> -->
  </div>

  
  <div class="row mb-3">
    <!-- 
    <a href="/publications/7_project/">
     -->
      
      <div class="col-sm-3">
          <img class="img-fluid" src="/assets/img/incluset.png" alt="" />
      </div>
      
      <div class="col-sm-9">
          <h4 class="card-title">IncluSet&#58; A Data Surfacing Repository for Accessibility Datasets</h4>
          <p class="card-text">Kacorri, H., <b>Dwivedi, U.</b>, Amancherla, S., Jha M., and Chanduka, R. IncluSet&#58; A Data Surfacing Repository for Accessibility Datasets. <i>International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '20).</i></p>
          
          
          <div class="row abbr ml-1 p-0 pubs">
           <div class="links">
                <a href="/pdfs/incluset.pdf" class="btn btn-sm z-depth-0 m-0" role="button" target="_blank">Poster <i class="fas fa-download"></i></a>
                
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                
                </div>
                
                  <div class="abstract hidden">
                    <p>Datasets and data sharing play an important role for innovation, benchmarking, mitigating bias, and understanding the complexity of real world AI-infused applications. However, there is a scarcity of available data generated by people with disabilities with the potential for training or evaluating machine learning models. This is partially due to smaller populations, disparate characteristics, lack of expertise for data annotation, as well as privacy concerns. Even when data are collected and are publicly available, it is often difcult to locate them. We present a novel data surfacing repository, called IncluSet, that allows researchers and the disability community to discover and link accessibility datasets. The repository is pre-populated with information about 139 existing datasets - 65 made publicly available, 25 available upon request, and 49 not shared by the authors but described in their manuscripts. More importantly, IncluSet is designed to expose existing and new dataset contributions so they may be discoverable through Google Dataset Search.</p>
                  </div>
                

          </div>
          <div class="row ml-1 mr-1 p-0 pubs">
              
            
        </div>
      </div>
    <!-- </a> -->
  </div>

  
  <div class="row mb-3">
    <!-- 
    <a href="/publications/assets21/">
     -->
      
      <div class="col-sm-3">
          <img class="img-fluid" src="/assets/img/inclusetproject.png" alt="A drawing showing the different datasets that were collected. Following are the depictions - Eye-tracking for children, typing patterns, hand gesture recognition, fingerspelling detector using images of hands, dysartic speech recognition where an adult is speaking to a mobile app, and sensors on feet of Blind people for tracing their path." />
      </div>
      
      <div class="col-sm-9">
          <h4 class="card-title">Sharing Practices for Datasets Related to Wellness, Accessibility, and Aging</h4>
          <p class="card-text">Kamikubo R., <b>Dwivedi, U.</b>, and Kacorri, H. Sharing Practices for Datasets Related to Wellness, Accessibility, and Aging <i>International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '21).</i></p>
          
          
          <div class="row abbr ml-1 p-0 pubs">
           <div class="links">
                <a href="https://arxiv.org/abs/2108.10665" class="btn btn-sm z-depth-0 m-0" role="button" target="_blank">Full Paper <i class="fas fa-download"></i></a>
                
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                
                </div>
                
                  <div class="abstract hidden">
                    <p>Iteratively building and testing machine learning models can help children develop creativity, flexibility, and comfort with machine learning and artificial intelligence. We explore how children use machine teaching interfaces with a team of 14 children (aged 7-13 years) and adult co-designers.Children trained image classifiers and tested each other's models for robustness. Our study illuminates how children reason about ML concepts, offering these insights for designing machine teaching experiences for children - (i) ML metrics (\eg confidence scores should be visible for experimentation; (ii) ML activities should enable children to exchange models for promoting reflection and pattern recognition; and (iii) the interface should allow quick data inspection (\eg images vs. gestures).</p>
                  </div>
                

          </div>
          <div class="row ml-1 mr-1 p-0 pubs">
              
            
        </div>
      </div>
    <!-- </a> -->
  </div>

  
  <div class="row mb-3">
    <!-- 
    <a href="/publications/8_project/">
     -->
      
      <div class="col-sm-3">
          <img class="img-fluid" src="/assets/img/dataset_examples.png" alt="Image" />
      </div>
      
      <div class="col-sm-9">
          <h4 class="card-title">Data Sharing in Wellness, Accessibility, and Aging</h4>
          <p class="card-text">Kacorri, H., <b>Dwivedi, U.</b>, Amancherla, S., Jha M., Kamibuko, R. and Chanduka, R. Data Sharing in Wellness, Accessibility, and Aging. <i>NeurIPS 2020 Workshop on Dataset Curation and Security (NeurIPS 2020).</i></p>
          
          
          <div class="row abbr ml-1 p-0 pubs">
           <div class="links">
                <a href="https://www.researchgate.net/publication/348844784_Data_Sharing_in_Wellness_Accessibility_and_Aging" class="btn btn-sm z-depth-0 m-0" role="button" target="_blank">Poster <i class="fas fa-download"></i></a>
                
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                
                </div>
                
                  <div class="abstract hidden">
                    <p>Datasets sourced from people with disabilities and older adults play an important role in innovation, benchmarking, and mitigating bias for both assistive and inclusive AI-infused applications. However, they are scarce. We conduct a systematic review of 137 accessibility datasets manually located across different disciplines over the last 35 years. Our analysis highlights how researchers navigate tensions between benefits and risks in data collection and sharing. We uncover patterns in data collection purpose, terminology, sample size, data types, and data sharing practices across communities of focus. We conclude by critically reflecting on challenges and opportunities related to locating and sharing accessibility datasets calling for technical, legal, and institutional privacy frameworks that are more attuned to concerns from these communities.</p>
                  </div>
                

          </div>
          <div class="row ml-1 mr-1 p-0 pubs">
              
            
        </div>
      </div>
    <!-- </a> -->
  </div>

  
  <div class="row mb-3">
    <!-- 
    <a href="/publications/6_project/">
     -->
      
      <div class="col-sm-3">
          <img class="img-fluid" src="/assets/img/optidwell.png" alt="" />
      </div>
      
      <div class="col-sm-9">
          <h4 class="card-title">OptiDwell&#58; Intelligent Adjustment of Dwell Click Time</h4>
          <p class="card-text">Nayyar A., <b>Dwivedi, U.</b>, Ahuja, K., Rajput, N., Nagar, S. and Dey, K. OptiDwell&#58; Intelligent Adjustment of Dwell Click Time. <i>International Conference on Intelligent User Interfaces (IUI) 2017.</i></p>
          
          
          <div class="row abbr ml-1 p-0 pubs">
           <div class="links">
                <a href="/pdfs/optidwell-intelligent-adjustment(3).pdf" class="btn btn-sm z-depth-0 m-0" role="button" target="_blank">Full Paper <i class="fas fa-download"></i></a>
                
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                
                </div>
                
                  <div class="abstract hidden">
                    <p>Gaze based navigation with digital screens offer a hands-free and touchless interaction, which is often useful in providing a hygienic interaction experience in a public kiosk scenario. The goodness of such a navigation system depends not only on the accuracy of detecting the eye gaze but also on the ability to determine whether a user is interested in clicking a button or is just looking at the button. The time for which a user needs to gaze at a particular button before it is considered as a click action is called the dwell time. In this paper, we explore intelligent adjustment of dwell times, where mouse click events on the buttons of a given application are emulated with user gaze. A constant dwell-time for all buttons and for all users may not provide an efficient and intuitive interface. We thereby propose a model to dynamically adjust dwell-time values used to emulate user mouse click events, exploiting the user’s experience with different portions of a given application. The adjustment happens at a per-user, per-button granularity, as a function of the user’s (a) prior usage experience of the given button within the application and (b) Midas touch characteristics for the given button. We propose OptiDwell, inspired by the action-value method based solutions to the Multi-Armed Bandits problem, for dwell click time adaptation. We experiment OptiDwell using an interactive TV channel browsing interface application, constituting of a mix of text and image buttons, over 10 computer-savvy users generating over 9000 click tasks. We observe significant improvement of user comfort level over the sessions, quantified by (a) improved (reduced) dwell times and (b) reduced number of Midas touches in spite of faster dwell-clicks, as high as 10-fold reduction in the best case. Our work is useful for creating an interface, with accurate, fast and comfortable dwell-clicks for each interface element (e.g., buttons), and each user.</p>
                  </div>
                

          </div>
          <div class="row ml-1 mr-1 p-0 pubs">
              
            
        </div>
      </div>
    <!-- </a> -->
  </div>

  
  <div class="row mb-3">
    <!-- 
    <a href="/publications/3_project/">
     -->
      
      <div class="col-sm-3">
          <img class="img-fluid" src="/assets/img/notecode.png" alt="" />
      </div>
      
      <div class="col-sm-9">
          <h4 class="card-title">Note Code&#58; A Tangible Music Programming Puzzle Tool</h4>
          <p class="card-text">Kumar, V., Dargan, T., <b>Dwivedi, U.</b> and Vijay, P. <i>Note Code&#58; A Tangible Music Programming Puzzle Tool.</i> International Conference on <i>Tangible, Embedded and Embodied Interaction (TEI) 2015.</i></p>
          
          
          <div class="row abbr ml-1 p-0 pubs">
           <div class="links">
                <a href="/pdfs/NoteCode_ACM_TEI_15.pdf" class="btn btn-sm z-depth-0 m-0" role="button" target="_blank">Poster <i class="fas fa-download"></i></a>
                
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                
                </div>
                
                  <div class="abstract hidden">
                    <p>We present the design of Note Code – a music programming puzzle game designed as a tangible device coupled with a Graphical User Interface (GUI). Tapping patterns and placing boxes in proximity enables programming these ‘note-boxes’ to store sets of notes, play them back and activate different subcomponents or neighboring boxes. This system provides users the opportunity to learn a variety of computational concepts, including functions, function calling and recursion, conditionals, as well as engage in composing music. The GUI adds a dimension of viewing the created programs and interacting with a set of puzzles that help discover the various computational concepts in the pursuit of creating target tunes, and optimizing the program made.</p>
                  </div>
                

          </div>
          <div class="row ml-1 mr-1 p-0 pubs">
              
            
        </div>
      </div>
    <!-- </a> -->
  </div>

  
  <div class="row mb-3">
    <!-- 
    <a href="/publications/4_project/">
     -->
      
      <div class="col-sm-3">
          <img class="img-fluid" src="/assets/img/visualmath.png" alt="" />
      </div>
      
      <div class="col-sm-9">
          <h4 class="card-title">Visual Math&#58; An automated visualization system for understanding math word problems</h4>
          <p class="card-text"><b>Dwivedi, U.</b>, Dey, P., Rajput, N.,Varkey B. Visual Math&#58; An automated visualization system for understanding math word problems.<i>International Conference on Intelligent User Interfaces (IUI) 2017.</i></p>
          
          
          <div class="row abbr ml-1 p-0 pubs">
           <div class="links">
                <a href="/pdfs/visualMath_cameraReady_iui17.pdf" class="btn btn-sm z-depth-0 m-0" role="button" target="_blank">Poster <i class="fas fa-download"></i></a>
                
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                
                </div>
                
                  <div class="abstract hidden">
                    <p>Math word problems are difficult for students to start with since they involve understanding the problem?s context and abstracting out its underlying mathematical operations. A visual understanding of the problem at hand can be very useful for the comprehension of the problem. We present a system VisualMath that uses machine learning tools and crafted visual logic to automatically generate appropriate visualizations from the text of the word-problems and solve it. We demonstrate the improvements in the understanding of math word-problems by conducting a user study and learning of meaning of relevant new words by students.</p>
                  </div>
                

          </div>
          <div class="row ml-1 mr-1 p-0 pubs">
              
            
        </div>
      </div>
    <!-- </a> -->
  </div>

  
  <div class="row mb-3">
    <!-- 
    <a href="/publications/9_project/">
     -->
      
      <div class="col-sm-3">
          <img class="img-fluid" src="/assets/img/vocab.png" alt="A flowchart describing the database of words that is used to predict the next best word to be taught to a child" />
      </div>
      
      <div class="col-sm-9">
          <h4 class="card-title">Using a Common Sense Knowledge Base to Auto Generate Multi-Dimensional Vocabulary Assessments</h4>
          <p class="card-text">Sharma Mittal, R., Nagar, S., Sharma, M., <b>Dwivedi, U.</b>, Dey, P., &amp; Kokku, R. (2018). Using a Common Sense Knowledge Base to Auto Generate Multi-Dimensional Vocabulary Assessments. <i>International Educational Data Mining Society.</i></p>
          
          
          <div class="row abbr ml-1 p-0 pubs">
           <div class="links">
                <a href="https://files.eric.ed.gov/fulltext/ED593200.pdf" class="btn btn-sm z-depth-0 m-0" role="button" target="_blank">Full Paper <i class="fas fa-download"></i></a>
                
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                
                </div>
                
                  <div class="abstract hidden">
                    <p>As education gets increasingly digitized, and intelligent tutoring systems gain commercial prominence, scalable assessment generation mechanisms become a critical requirement for enabling increased learning outcomes. Assessments provide a way to measure learners' level of understanding and difficulty, and personalize their learning. There have been separate efforts in di erent areas to solve this by looking at different parts of the problem. This paper is a  rst effort to bring together techniques from diverse areas such as knowledge representation and reasoning, machine learning, inference on graphs, and pedagogy to generate automated assessments at scale. In this paper, we speci cally address the problem of Multiple Choice Question (MCQ) generation for vocabulary learning assessments, specially catered to young learners (YL). We evaluate the e cacy of our approach by asking human annotators to annotate the questions generated by the system based on relevance. We also compare our approach with one baseline model and report high usability of MCQs generated by our system compared to the baseline.</p>
                  </div>
                

          </div>
          <div class="row ml-1 mr-1 p-0 pubs">
              
            
        </div>
      </div>
    <!-- </a> -->
  </div>

  
  <div class="row mb-3">
    <!-- 
    <a href="/publications/5_project/">
     -->
      
      <div class="col-sm-3">
          <img class="img-fluid" src="/assets/img/eyamkayo.png" alt="" />
      </div>
      
      <div class="col-sm-9">
          <h4 class="card-title">EyamKayo&#58; Interactive Gaze and Facial Expression Captcha</h4>
          <p class="card-text"><b>Dwivedi, U.</b>, Ahuja, K., Islam, R., Bhabhuria F., Nagar, S. and Dey, K. EyamKayo&#58; Interactive Gaze and Facial Expression Captcha. <i>International Conference on Intelligent User Interfaces (IUI) 2017.</i></p>
          
          
          <div class="row abbr ml-1 p-0 pubs">
           <div class="links">
                <a href="/pdfs/eyamkayo-interactive-gaze.pdf" class="btn btn-sm z-depth-0 m-0" role="button" target="_blank">Poster <i class="fas fa-download"></i></a>
                
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                
                </div>
                
                  <div class="abstract hidden">
                    <p>This paper introduces EyamKayo, a first-of-its-kind interactive CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart), using eye gaze and facial expression based human interactions, to better distinguish humans from software robots. Our system generates a sequence of instructions, asking the user to follow a controlled sequence of facial expressions. We evaluate user comfort and system usability, and validate using usability tests.</p>
                  </div>
                

          </div>
          <div class="row ml-1 mr-1 p-0 pubs">
              
            
        </div>
      </div>
    <!-- </a> -->
  </div>

  
  <div class="row mb-3">
    <!-- 
    <a href="/publications/1_project/">
     -->
      
      <div class="col-sm-3">
          <img class="img-fluid" src="/assets/img/machinetask.png" alt="" />
      </div>
      
      <div class="col-sm-9">
          <h4 class="card-title">On optimizing human-machine task assignments</h4>
          <p class="card-text">Veit, A et al. On optimizing human-machine task assignments. <i>AAAI Conference on Human Computation and Crowdsourcing (HCOMP) 2015.</i></p>
          
          
          <div class="row abbr ml-1 p-0 pubs">
           <div class="links">
                <a href="/pdfs/On_Optimizing_Human-Machine_Task_Assignments.pdf" class="btn btn-sm z-depth-0 m-0" role="button" target="_blank">Short Paper <i class="fas fa-download"></i></a>
                
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                
                </div>
                
                  <div class="abstract hidden">
                    <p>When crowdsourcing systems are used in combination with machine inference systems in the real world, they benefit the most when the machine system is deeply integrated with the crowd workers. However, if researchers wish to integrate the crowd with “off-the-shelf” machine classifiers, this deep integration is not always possible. This work explores two strategies to increase accuracy and decrease cost under this setting. First, we show that reordering tasks presented to the human can create a significant accuracy improvement. Further, we show that greedily choosing parameters to maximize machine accuracy is sub-optimal, and joint optimization of the combined system improves performance.</p>
                  </div>
                

          </div>
          <div class="row ml-1 mr-1 p-0 pubs">
              
            
        </div>
      </div>
    <!-- </a> -->
  </div>

  
  <div class="row mb-3">
    <!-- 
    <a href="/publications/2_project/">
     -->
      
      <div class="col-sm-3">
          <img class="img-fluid" src="/assets/img/compliance.png" alt="" />
      </div>
      
      <div class="col-sm-9">
          <h4 class="card-title">Enabling Compliance of Environmental Conditions</h4>
          <p class="card-text"><b>Dwivedi, U.</b> and Dasgupta, A. <i>Enabling Compliance of Environmental Conditions.</i> Computing for Development (ACM DEV '15).</p>
          
          
          <div class="row abbr ml-1 p-0 pubs">
           <div class="links">
                <a href="/pdfs/Environmental_Compliance_ACM_DEV_15.pdf" class="btn btn-sm z-depth-0 m-0" role="button" target="_blank">Poster <i class="fas fa-download"></i></a>
                
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
                
                </div>
                
                  <div class="abstract hidden">
                    <p>Industrial projects in India have to agree to specific sets of environmental conditions in order to function. Lack of compliance with these conditions results both in irreversible damage to the local environment as well as conflicts among the industry and the local community. Our aim is to provide a system that raises general awareness in the local community about the environmental conditions in vogue among the nearby industries so that compliance violations can be reported early on. We outline work in progress to mine the text of the clearance conditions and build a searchable mapping system that can answer various queries about these conditions.</p>
                  </div>
                

          </div>
          <div class="row ml-1 mr-1 p-0 pubs">
              
            
        </div>
      </div>
    <!-- </a> -->
  </div>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container justify-content-center">
    &copy; Copyright 2024 Utkarsh  Dwivedi.
    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P93BWQVTWE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-P93BWQVTWE');
</script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
